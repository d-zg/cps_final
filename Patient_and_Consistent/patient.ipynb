{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e58b158-6235-4a24-9aaf-13b1e2ba29c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T02:06:56.511538Z",
     "iopub.status.busy": "2025-04-10T02:06:56.511315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Training student (SqueezeNet) with teacher (ResNet18) distillation on enriched CIFAR-10 data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: 100%|██████████| 586/586 [00:23<00:00, 25.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] - Train Loss: 5.6375, Train Acc: 14.34% | Val Loss: 2.2623, Val Acc: 24.74%\n",
      "New best student model found at epoch 1 with Val Acc: 24.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/200: 100%|██████████| 586/586 [00:22<00:00, 25.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/200] - Train Loss: 4.8802, Train Acc: 19.40% | Val Loss: 1.8642, Val Acc: 37.61%\n",
      "New best student model found at epoch 2 with Val Acc: 37.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/200: 100%|██████████| 586/586 [00:23<00:00, 25.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/200] - Train Loss: 4.3838, Train Acc: 22.69% | Val Loss: 1.7658, Val Acc: 43.98%\n",
      "New best student model found at epoch 3 with Val Acc: 43.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/200: 100%|██████████| 586/586 [00:23<00:00, 25.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/200] - Train Loss: 4.0163, Train Acc: 23.56% | Val Loss: 1.7773, Val Acc: 45.73%\n",
      "New best student model found at epoch 4 with Val Acc: 45.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/200: 100%|██████████| 586/586 [00:23<00:00, 24.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/200] - Train Loss: 3.7601, Train Acc: 25.87% | Val Loss: 1.5069, Val Acc: 50.88%\n",
      "New best student model found at epoch 5 with Val Acc: 50.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/200: 100%|██████████| 586/586 [00:22<00:00, 25.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/200] - Train Loss: 3.6073, Train Acc: 27.12% | Val Loss: 1.5855, Val Acc: 53.32%\n",
      "New best student model found at epoch 6 with Val Acc: 53.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/200: 100%|██████████| 586/586 [00:23<00:00, 24.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/200] - Train Loss: 3.4166, Train Acc: 27.01% | Val Loss: 1.4784, Val Acc: 55.56%\n",
      "New best student model found at epoch 7 with Val Acc: 55.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/200: 100%|██████████| 586/586 [00:23<00:00, 25.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/200] - Train Loss: 3.3251, Train Acc: 28.62% | Val Loss: 1.4103, Val Acc: 56.61%\n",
      "New best student model found at epoch 8 with Val Acc: 56.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/200: 100%|██████████| 586/586 [00:23<00:00, 25.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/200] - Train Loss: 3.2460, Train Acc: 29.44% | Val Loss: 1.3726, Val Acc: 58.44%\n",
      "New best student model found at epoch 9 with Val Acc: 58.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/200: 100%|██████████| 586/586 [00:23<00:00, 24.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200] - Train Loss: 3.1164, Train Acc: 29.24% | Val Loss: 1.3796, Val Acc: 59.99%\n",
      "New best student model found at epoch 10 with Val Acc: 59.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/200: 100%|██████████| 586/586 [00:23<00:00, 24.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/200] - Train Loss: 3.0353, Train Acc: 30.00% | Val Loss: 1.2708, Val Acc: 62.47%\n",
      "New best student model found at epoch 11 with Val Acc: 62.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/200: 100%|██████████| 586/586 [00:22<00:00, 25.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/200] - Train Loss: 2.9050, Train Acc: 29.59% | Val Loss: 1.2658, Val Acc: 63.94%\n",
      "New best student model found at epoch 12 with Val Acc: 63.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/200: 100%|██████████| 586/586 [00:23<00:00, 25.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/200] - Train Loss: 2.8105, Train Acc: 30.88% | Val Loss: 1.2570, Val Acc: 63.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/200: 100%|██████████| 586/586 [00:23<00:00, 24.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/200] - Train Loss: 2.7521, Train Acc: 30.78% | Val Loss: 1.3240, Val Acc: 64.04%\n",
      "New best student model found at epoch 14 with Val Acc: 64.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/200:  88%|████████▊ | 518/586 [00:20<00:02, 25.32it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, ConcatDataset, random_split\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from contextlib import redirect_stdout\n",
    "import numpy as np\n",
    "\n",
    "# ----- Define Tee class for logging -----\n",
    "class Tee(object):\n",
    "    def __init__(self, *fileobjects):\n",
    "        self.fileobjects = fileobjects\n",
    "    def write(self, text):\n",
    "        for f in self.fileobjects:\n",
    "            f.write(text)\n",
    "            f.flush()\n",
    "    def flush(self):\n",
    "        for f in self.fileobjects:\n",
    "            f.flush()\n",
    "\n",
    "# ----- Specify paths for saving logs, metrics, and models -----\n",
    "log_file_path = \"./logs/cifar10_training_log.txt\"\n",
    "metrics_file_path = \"./metrics/cifar10_training_metrics.json\"\n",
    "best_model_path = \"./models/cifar10_best_student_model.pth\"\n",
    "save_path = \"./models/cifar10_student_model\"\n",
    "teacher_checkpoint = \"/notebooks/Resnet18/models/cifar10_best_model\"  # Pre-trained teacher\n",
    "\n",
    "# ----- Define Data Augmentations for Enriched Training Data -----\n",
    "# Note: we include normalization so that images are in a standard range.\n",
    "norm = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "transform_a = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    norm\n",
    "])\n",
    "\n",
    "transform_b = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    norm\n",
    "])\n",
    "\n",
    "transform_c = transforms.Compose([\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    norm\n",
    "])\n",
    "\n",
    "# For validation/testing, use a simple transform (tensor conversion and normalization)\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    norm\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 training set three times with different augmentations.\n",
    "dataset_a = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_a)\n",
    "dataset_b = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_b)\n",
    "dataset_c = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_c)\n",
    "\n",
    "# Combine the three augmented datasets into one enriched dataset.\n",
    "enriched_dataset = ConcatDataset([dataset_a, dataset_b, dataset_c])\n",
    "\n",
    "# For validation and test, use the standard dataset with eval_transform.\n",
    "full_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=eval_transform)\n",
    "train_size = int(0.8 * len(full_trainset))\n",
    "val_size = len(full_trainset) - train_size\n",
    "trainset_eval, valset = random_split(full_trainset, [train_size, val_size])\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=eval_transform)\n",
    "\n",
    "# ----- Define Mixup Function and Custom Collate -----\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    \"\"\"Return mixed inputs, paired labels, and mixing coefficient.\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1.0\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_collate_fn(batch, alpha=0.4):\n",
    "    \"\"\"\n",
    "    Each element in 'batch' is a tuple (image, label).\n",
    "    Stack images and labels, then apply mixup.\n",
    "    \"\"\"\n",
    "    images, labels = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    labels = torch.tensor(labels)\n",
    "    mixed_images, labels_a, labels_b, lam = mixup_data(images, labels, alpha)\n",
    "    return mixed_images, labels_a, labels_b, lam\n",
    "\n",
    "# Create DataLoader for the enriched training dataset with mixup applied on the fly.\n",
    "train_loader = DataLoader(enriched_dataset, batch_size=256, shuffle=True, num_workers=2,\n",
    "                          collate_fn=lambda b: mixup_collate_fn(b, alpha=0.4))\n",
    "\n",
    "# For validation and test, use standard DataLoaders.\n",
    "val_loader = DataLoader(valset, batch_size=256, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(testset, batch_size=256, shuffle=False, num_workers=2)\n",
    "\n",
    "# ----- Prepare Teacher and Student Models -----\n",
    "# Teacher: ResNet18 modified for CIFAR-10.\n",
    "teacher = models.resnet18(weights=None)\n",
    "teacher.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "teacher.maxpool = nn.Identity()\n",
    "num_ftrs = teacher.fc.in_features\n",
    "teacher.fc = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "# Student: SqueezeNet (we use squeezenet1_1) modified for CIFAR-10.\n",
    "student = models.squeezenet1_1(pretrained=False)\n",
    "# SqueezeNet's classifier is a Sequential: (Dropout, Conv2d, ReLU, AvgPool2d)\n",
    "student.classifier[1] = nn.Conv2d(512, 10, kernel_size=(1,1), stride=(1,1))\n",
    "student.num_classes = 10\n",
    "\n",
    "# ----- Device and Checkpoints Setup -----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 200\n",
    "learning_rate = 0.001\n",
    "checkpoint_frequency = 10\n",
    "start_epoch = 0\n",
    "start_checkpoint = None  # Optionally, resume student training checkpoint\n",
    "\n",
    "# Load teacher checkpoint (pre-trained on CIFAR-10) if available.\n",
    "if os.path.exists(teacher_checkpoint):\n",
    "    teacher_state = torch.load(teacher_checkpoint, map_location=device)\n",
    "    teacher.load_state_dict(teacher_state)\n",
    "else:\n",
    "    print(\"Warning: Teacher checkpoint not found. Teacher model may be untrained.\")\n",
    "\n",
    "teacher.to(device)\n",
    "teacher.eval()  # Teacher is fixed during distillation.\n",
    "for param in teacher.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Optionally resume student training from a checkpoint.\n",
    "if start_checkpoint:\n",
    "    student_state = torch.load(start_checkpoint, map_location=device)\n",
    "    student.load_state_dict(student_state)\n",
    "student.to(device)\n",
    "\n",
    "# ----- Setup Loss Functions and Optimizer -----\n",
    "# Distillation loss: KL divergence with temperature scaling.\n",
    "kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "# You can also include standard CE loss if desired.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(student.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "temperature = 4.0\n",
    "\n",
    "# ----- Initialize Metrics Storage -----\n",
    "metrics = {\n",
    "    \"epochs\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": []\n",
    "}\n",
    "best_val_acc = 0.0\n",
    "best_epoch = None\n",
    "\n",
    "# ----- Setup Logging (Tee and TensorBoard) -----\n",
    "log_capture = io.StringIO()\n",
    "tee = Tee(sys.stdout, log_capture)  # Prints and captures output\n",
    "writer = SummaryWriter(log_dir='./runs/cifar10_distillation_experiment')\n",
    "\n",
    "with redirect_stdout(tee):\n",
    "    print(\"Training student (SqueezeNet) with teacher (ResNet18) distillation on enriched CIFAR-10 data...\")\n",
    "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "        student.train()\n",
    "        running_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        \n",
    "        # Save a checkpoint at the specified frequency.\n",
    "        if epoch % checkpoint_frequency == 0:\n",
    "            torch.save(student.state_dict(), f\"{save_path}_{epoch}.pth\")\n",
    "        \n",
    "        # Training loop with tqdm progress bar.\n",
    "        for mixed_inputs, labels_a, labels_b, lam in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            # Move data to device.\n",
    "            mixed_inputs = mixed_inputs.to(device)\n",
    "            # For mixup, we use original labels (two sets) and later combine losses if needed.\n",
    "            labels_a = labels_a.to(device)\n",
    "            labels_b = labels_b.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass through the teacher on the same mixup inputs.\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(mixed_inputs)\n",
    "            # Forward pass through the student.\n",
    "            student_logits = student(mixed_inputs)\n",
    "            \n",
    "            # Compute distillation loss using KL divergence with temperature scaling.\n",
    "            teacher_soft = torch.softmax(teacher_logits / temperature, dim=1)\n",
    "            student_log_soft = torch.log_softmax(student_logits / temperature, dim=1)\n",
    "            loss_distill = (temperature * temperature) * kl_loss(student_log_soft, teacher_soft)\n",
    "            \n",
    "            # Optionally, one can add cross-entropy loss on original targets:\n",
    "            # Compute CE loss on both sets and combine using the mixup coefficient:\n",
    "            # loss_ce = lam * criterion(student_logits, labels_a) + (1 - lam) * criterion(student_logits, labels_b)\n",
    "            # loss = loss_distill * lambda_distill + loss_ce * lambda_ce\n",
    "            # For this script, we use only the KL divergence distillation loss.\n",
    "            loss = loss_distill\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * mixed_inputs.size(0)\n",
    "            total += mixed_inputs.size(0)\n",
    "            # For training accuracy, use student logits to predict hard labels.\n",
    "            _, predicted = student_logits.max(1)\n",
    "            # For simplicity, assume that the true label is the one chosen with higher mixing weight.\n",
    "            # (In practice, you might use a weighted combination; here we just use labels_a.)\n",
    "            correct += predicted.eq(labels_a).sum().item()\n",
    "        \n",
    "        train_epoch_loss = running_loss / total\n",
    "        train_epoch_acc = 100. * correct / total\n",
    "\n",
    "        # ----- Validate on the Validation Set (without mixup) -----\n",
    "        student.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = student(inputs)\n",
    "                loss = criterion(outputs, labels)  # Use standard cross-entropy for evaluation\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        val_epoch_loss = val_loss / val_total\n",
    "        val_epoch_acc = 100. * val_correct / val_total\n",
    "\n",
    "        metrics[\"epochs\"].append(epoch + 1)\n",
    "        metrics[\"train_loss\"].append(train_epoch_loss)\n",
    "        metrics[\"train_acc\"].append(train_epoch_acc)\n",
    "        metrics[\"val_loss\"].append(val_epoch_loss)\n",
    "        metrics[\"val_acc\"].append(val_epoch_acc)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{start_epoch + num_epochs}] - Train Loss: {train_epoch_loss:.4f}, Train Acc: {train_epoch_acc:.2f}% | Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.2f}%\")\n",
    "        \n",
    "        # TensorBoard logging.\n",
    "        writer.add_scalar('Loss/Train', train_epoch_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Train', train_epoch_acc, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_epoch_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_epoch_acc, epoch)\n",
    "        \n",
    "        # Save the best model based on validation accuracy.\n",
    "        if val_epoch_acc > best_val_acc:\n",
    "            best_val_acc = val_epoch_acc\n",
    "            best_epoch = epoch + 1\n",
    "            torch.save(student.state_dict(), best_model_path)\n",
    "            print(f\"New best student model found at epoch {epoch+1} with Val Acc: {val_epoch_acc:.2f}%\")\n",
    "        \n",
    "        # Save metrics JSON after each epoch.\n",
    "        with open(metrics_file_path, \"w\") as f:\n",
    "            json.dump(metrics, f, indent=4)\n",
    "    \n",
    "    print(\"Training complete.\")\n",
    "    torch.save(student.state_dict(), save_path + \"_final.pth\")\n",
    "\n",
    "# Close the TensorBoard writer.\n",
    "writer.close()\n",
    "\n",
    "# Write captured logs to the specified log file.\n",
    "with open(log_file_path, \"w\") as f:\n",
    "    f.write(log_capture.getvalue())\n",
    "\n",
    "print(\"Training log captured and saved to\", log_file_path)\n",
    "print(\"Training metrics saved to\", metrics_file_path)\n",
    "print(\"Best student model saved from epoch\", best_epoch, \"with validation accuracy of\", best_val_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
