{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eaca5d-05f0-4362-8fbc-b937e80ef28b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T21:19:03.835269Z",
     "iopub.status.busy": "2025-04-21T21:19:03.834979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Generating adversarial examples for 5000 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring teacher confidences: 100%|██████████| 50000/50000 [01:42<00:00, 486.50it/s]\n",
      "Generating adversarial samples: 100%|██████████| 5000/5000 [04:16<00:00, 19.48it/s]\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training student (SqueezeNet) with teacher (ResNet18) distillation and adversarial sample augmentation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: 100%|██████████| 606/606 [00:22<00:00, 27.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] - Train Loss: 6.6689, Train Acc: 23.07% | Val Loss: 1.9313, Val Acc: 35.72%\n",
      "New best student model found at epoch 1 with Val Acc: 35.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/200: 100%|██████████| 606/606 [00:22<00:00, 27.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/200] - Train Loss: 5.2963, Train Acc: 37.17% | Val Loss: 1.7059, Val Acc: 46.49%\n",
      "New best student model found at epoch 2 with Val Acc: 46.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/200: 100%|██████████| 606/606 [00:21<00:00, 27.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/200] - Train Loss: 4.7343, Train Acc: 42.58% | Val Loss: 1.6550, Val Acc: 50.96%\n",
      "New best student model found at epoch 3 with Val Acc: 50.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/200: 100%|██████████| 606/606 [00:21<00:00, 27.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/200] - Train Loss: 4.3593, Train Acc: 46.27% | Val Loss: 1.6686, Val Acc: 52.33%\n",
      "New best student model found at epoch 4 with Val Acc: 52.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/200: 100%|██████████| 606/606 [00:22<00:00, 27.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/200] - Train Loss: 4.1254, Train Acc: 48.65% | Val Loss: 1.6043, Val Acc: 55.18%\n",
      "New best student model found at epoch 5 with Val Acc: 55.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/200: 100%|██████████| 606/606 [00:22<00:00, 27.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/200] - Train Loss: 3.9503, Train Acc: 50.39% | Val Loss: 1.4978, Val Acc: 58.78%\n",
      "New best student model found at epoch 6 with Val Acc: 58.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/200: 100%|██████████| 606/606 [00:32<00:00, 18.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/200] - Train Loss: 3.7856, Train Acc: 52.11% | Val Loss: 1.5017, Val Acc: 59.08%\n",
      "New best student model found at epoch 7 with Val Acc: 59.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/200: 100%|██████████| 606/606 [00:32<00:00, 18.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/200] - Train Loss: 3.6116, Train Acc: 53.56% | Val Loss: 1.5060, Val Acc: 59.49%\n",
      "New best student model found at epoch 8 with Val Acc: 59.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/200: 100%|██████████| 606/606 [00:33<00:00, 18.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/200] - Train Loss: 3.4742, Train Acc: 54.68% | Val Loss: 1.2700, Val Acc: 64.58%\n",
      "New best student model found at epoch 9 with Val Acc: 64.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/200: 100%|██████████| 606/606 [00:34<00:00, 17.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200] - Train Loss: 3.3384, Train Acc: 55.75% | Val Loss: 1.3675, Val Acc: 63.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/200: 100%|██████████| 606/606 [00:40<00:00, 14.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/200] - Train Loss: 3.2325, Train Acc: 57.07% | Val Loss: 1.3625, Val Acc: 64.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/200: 100%|██████████| 606/606 [00:40<00:00, 14.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/200] - Train Loss: 3.1282, Train Acc: 57.84% | Val Loss: 1.2307, Val Acc: 67.28%\n",
      "New best student model found at epoch 12 with Val Acc: 67.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/200: 100%|██████████| 606/606 [00:40<00:00, 15.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/200] - Train Loss: 3.0334, Train Acc: 58.61% | Val Loss: 1.1642, Val Acc: 68.98%\n",
      "New best student model found at epoch 13 with Val Acc: 68.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/200: 100%|██████████| 606/606 [00:40<00:00, 14.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/200] - Train Loss: 2.9676, Train Acc: 59.11% | Val Loss: 1.1606, Val Acc: 69.03%\n",
      "New best student model found at epoch 14 with Val Acc: 69.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/200: 100%|██████████| 606/606 [00:39<00:00, 15.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/200] - Train Loss: 2.9081, Train Acc: 59.54% | Val Loss: 1.1985, Val Acc: 69.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/200: 100%|██████████| 606/606 [00:40<00:00, 14.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/200] - Train Loss: 2.8493, Train Acc: 60.50% | Val Loss: 1.1446, Val Acc: 70.07%\n",
      "New best student model found at epoch 16 with Val Acc: 70.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/200:  65%|██████▌   | 395/606 [00:26<00:14, 14.88it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Dataset, random_split\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from contextlib import redirect_stdout\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# Logging Utilities\n",
    "# -------------------------------\n",
    "class Tee(object):\n",
    "    def __init__(self, *fileobjects):\n",
    "        self.fileobjects = fileobjects\n",
    "    def write(self, text):\n",
    "        for f in self.fileobjects:\n",
    "            f.write(text)\n",
    "            f.flush()\n",
    "    def flush(self):\n",
    "        for f in self.fileobjects:\n",
    "            f.flush()\n",
    "\n",
    "# -------------------------------\n",
    "# Paths and Checkpoints\n",
    "# -------------------------------\n",
    "log_file_path = \"./logs/cifar10_training_log.txt\"\n",
    "metrics_file_path = \"./metrics/cifar10_training_metrics.json\"\n",
    "best_model_path = \"./models/cifar10_best_student_model.pth\"\n",
    "save_path = \"./models/cifar10_student_model\"\n",
    "teacher_checkpoint = \"/notebooks/Resnet18/models/cifar10_best_model\"  # Pre-trained teacher\n",
    "\n",
    "# -------------------------------\n",
    "# Data Augmentations (No Mixup)\n",
    "# -------------------------------\n",
    "norm = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "# Three different augmentation pipelines for the enriched dataset.\n",
    "transform_a = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    norm\n",
    "])\n",
    "\n",
    "transform_b = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    norm\n",
    "])\n",
    "\n",
    "transform_c = transforms.Compose([\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    norm\n",
    "])\n",
    "\n",
    "# For evaluation (and also for generating adversarial examples), use a simple transform.\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    norm\n",
    "])\n",
    "\n",
    "# -------------------------------\n",
    "# Load CIFAR-10 Datasets\n",
    "# -------------------------------\n",
    "# Enriched training dataset: three copies with different augmentations.\n",
    "dataset_a = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_a)\n",
    "dataset_b = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_b)\n",
    "dataset_c = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_c)\n",
    "enriched_dataset = ConcatDataset([dataset_a, dataset_b, dataset_c])\n",
    "\n",
    "# Full training dataset with eval_transform.\n",
    "full_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=eval_transform)\n",
    "\n",
    "# Split full training set for validation.\n",
    "train_size = int(0.8 * len(full_trainset))\n",
    "val_size = len(full_trainset) - train_size\n",
    "trainset_eval, valset = random_split(full_trainset, [train_size, val_size])\n",
    "\n",
    "# Standard test set.\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=eval_transform)\n",
    "\n",
    "# -------------------------------\n",
    "# Define the Adversarial Example Generator\n",
    "# -------------------------------\n",
    "def generate_adversarial_trajectory(image, teacher, num_steps=10, step_size=0.001):\n",
    "    \"\"\"\n",
    "    Generate a trajectory of adversarial images targeting a fixed class (the second-highest logit).\n",
    "    \n",
    "    Args:\n",
    "      image (torch.Tensor): the normalized image tensor.\n",
    "      teacher (torch.nn.Module): the teacher model.\n",
    "      num_steps (int): number of gradient steps.\n",
    "      step_size (float): step size for each update.\n",
    "    \n",
    "    Returns:\n",
    "      List[torch.Tensor]: a list of adversarial images (one per step).\n",
    "    \"\"\"\n",
    "    # Clone image and enable gradients.\n",
    "    x_adv = image.clone().detach().to(device)\n",
    "    x_adv.requires_grad = True\n",
    "    adversarial_images = []\n",
    "    \n",
    "    # Determine fixed target using the second highest logit.\n",
    "    with torch.no_grad():\n",
    "        logits_init = teacher(x_adv.unsqueeze(0))\n",
    "        sorted_logits, sorted_indices = torch.sort(logits_init, descending=True)\n",
    "        fixed_target = sorted_indices[0, 1].item()  # second highest logit index\n",
    "    \n",
    "    # Generate adversarial trajectory.\n",
    "    for step in range(num_steps):\n",
    "        logits = teacher(x_adv.unsqueeze(0))\n",
    "        loss = -logits[0, fixed_target]  # maximize fixed target logit (i.e. minimize negative)\n",
    "        \n",
    "        teacher.zero_grad()\n",
    "        if x_adv.grad is not None:\n",
    "            x_adv.grad.zero_()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update adversarial image using the sign of the gradient.\n",
    "        with torch.no_grad():\n",
    "            x_adv = x_adv - step_size * x_adv.grad.sign()\n",
    "        x_adv.requires_grad = True\n",
    "        adversarial_images.append(x_adv.clone().detach())\n",
    "        \n",
    "    return adversarial_images\n",
    "\n",
    "# -------------------------------\n",
    "# Create Adversarial Dataset\n",
    "# -------------------------------\n",
    "# We now generate adversarial samples on a random subset of the full training set.\n",
    "# Here, we generate one adversarial example per selected image by taking the final sample of the adversarial trajectory.\n",
    "# Adjust adv_fraction as needed (e.g., 0.1 means 10% of full_trainset).\n",
    "adv_fraction = 0.1  \n",
    "num_adv_samples = int(len(full_trainset) * adv_fraction)\n",
    "print(f\"Generating adversarial examples for {num_adv_samples} samples...\")\n",
    "\n",
    "# Ensure the teacher model is set up before using it.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -------------------------------\n",
    "# Prepare Teacher Model (ResNet18 for CIFAR-10)\n",
    "# -------------------------------\n",
    "teacher = models.resnet18(weights=None)\n",
    "# Adjust first conv layer and maxpool as appropriate for CIFAR-10.\n",
    "teacher.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "teacher.maxpool = nn.Identity()\n",
    "num_ftrs = teacher.fc.in_features\n",
    "teacher.fc = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "# Load teacher checkpoint if available.\n",
    "if os.path.exists(teacher_checkpoint):\n",
    "    teacher_state = torch.load(teacher_checkpoint, map_location=device)\n",
    "    teacher.load_state_dict(teacher_state)\n",
    "else:\n",
    "    print(\"Warning: Teacher checkpoint not found. Using an untrained teacher model.\")\n",
    "\n",
    "teacher.to(device)\n",
    "# Stage 1: score all training samples by their 2nd‐highest logit\n",
    "adv_samples = []\n",
    "teacher.eval()\n",
    "second_logits = []\n",
    "for img, _ in tqdm(full_trainset, desc=\"Scoring teacher confidences\"):\n",
    "    with torch.no_grad():\n",
    "        logits = teacher(img.unsqueeze(0).to(device))\n",
    "    top2, _ = torch.topk(logits, k=2, dim=1)\n",
    "    second_logits.append(top2[0, 1].item())\n",
    "second_logits = np.array(second_logits)\n",
    "\n",
    "# Stage 2: pick the top‐m “hesitant” inputs\n",
    "m = num_adv_samples\n",
    "selected_indices = np.argsort(-second_logits)[:m]\n",
    "\n",
    "# (keep your num_attack_steps and attack_step_size definitions here)\n",
    "num_attack_steps = 10\n",
    "attack_step_size = 0.001\n",
    "\n",
    "\n",
    "for idx in tqdm(selected_indices, desc=\"Generating adversarial samples\"):\n",
    "    image, label = full_trainset[idx]\n",
    "    # Generate the trajectory.\n",
    "    adv_traj = generate_adversarial_trajectory(image, teacher, num_steps=num_attack_steps, step_size=attack_step_size)\n",
    "    # Choose the final adversarial sample from the trajectory.\n",
    "    adv_image = adv_traj[-1]\n",
    "    # Append tuple (adv_image, label). Here we keep the original label.\n",
    "    adv_samples.append((adv_image.cpu(), label))\n",
    "\n",
    "# Create a simple Dataset for adversarial samples.\n",
    "class AdversarialDataset(Dataset):\n",
    "    def __init__(self, sample_list):\n",
    "        self.samples = sample_list\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]  # returns (image, label)\n",
    "\n",
    "adv_dataset = AdversarialDataset(adv_samples)\n",
    "\n",
    "# Combine the enriched dataset with the adversarial dataset.\n",
    "combined_dataset = ConcatDataset([enriched_dataset, adv_dataset])\n",
    "\n",
    "# -------------------------------\n",
    "# DataLoaders (No mixup used)\n",
    "# -------------------------------\n",
    "train_loader = DataLoader(combined_dataset, batch_size=256, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(valset, batch_size=256, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(testset, batch_size=256, shuffle=False, num_workers=2)\n",
    "\n",
    "# -------------------------------\n",
    "# Prepare Teacher and Student Models for Distillation\n",
    "# -------------------------------\n",
    "# Teacher is already defined above. Teacher remains fixed.\n",
    "# Prepare the student model (SqueezeNet1_1 adapted for CIFAR-10).\n",
    "student = models.squeezenet1_1(pretrained=False)\n",
    "student.classifier[1] = nn.Conv2d(512, 10, kernel_size=(1,1), stride=(1,1))\n",
    "student.num_classes = 10\n",
    "student.to(device)\n",
    "\n",
    "# Optionally resume student training from a checkpoint by setting start_checkpoint.\n",
    "start_checkpoint = None\n",
    "if start_checkpoint:\n",
    "    student_state = torch.load(start_checkpoint, map_location=device)\n",
    "    student.load_state_dict(student_state)\n",
    "\n",
    "# Freeze teacher parameters.\n",
    "for param in teacher.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# -------------------------------\n",
    "# Training Hyperparameters and Loss Setup\n",
    "# -------------------------------\n",
    "num_epochs = 200\n",
    "learning_rate = 0.001\n",
    "checkpoint_frequency = 10\n",
    "\n",
    "# Distillation loss: KL divergence with temperature scaling.\n",
    "kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "# Optionally, you can also include standard cross-entropy loss.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(student.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "temperature = 4.0\n",
    "\n",
    "# CIFAR-10 class names (for logging if desired).\n",
    "cifar_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "                 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# -------------------------------\n",
    "# Metrics and Logging Setup\n",
    "# -------------------------------\n",
    "metrics = {\n",
    "    \"epochs\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": []\n",
    "}\n",
    "best_val_acc = 0.0\n",
    "best_epoch = None\n",
    "log_capture = io.StringIO()\n",
    "tee = Tee(sys.stdout, log_capture)\n",
    "writer = SummaryWriter(log_dir='./runs/cifar10_distillation_experiment')\n",
    "\n",
    "# -------------------------------\n",
    "# Training Loop: Knowledge Distillation with Adversarial Augmentation\n",
    "# -------------------------------\n",
    "with redirect_stdout(tee):\n",
    "    print(\"Training student (SqueezeNet) with teacher (ResNet18) distillation and adversarial sample augmentation...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        student.train()\n",
    "        running_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        # Save checkpoint at specified frequency.\n",
    "        if epoch % checkpoint_frequency == 0:\n",
    "            torch.save(student.state_dict(), f\"{save_path}_{epoch}.pth\")\n",
    "\n",
    "        # Standard training loop over batches (no mixup).\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass through teacher (teacher outputs are used as soft targets).\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(inputs)\n",
    "            # Forward pass through student.\n",
    "            student_logits = student(inputs)\n",
    "            \n",
    "            # Compute distillation loss with temperature scaling.\n",
    "            teacher_soft = torch.softmax(teacher_logits / temperature, dim=1)\n",
    "            student_log_soft = torch.log_softmax(student_logits / temperature, dim=1)\n",
    "            loss_distill = (temperature ** 2) * kl_loss(student_log_soft, teacher_soft)\n",
    "            loss = loss_distill\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            total += inputs.size(0)\n",
    "            _, predicted = student_logits.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        train_epoch_loss = running_loss / total\n",
    "        train_epoch_acc = 100. * correct / total\n",
    "\n",
    "        # ----- Validation Loop (standard CIFAR-10 evaluation using cross-entropy loss) -----\n",
    "        student.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = student(inputs)\n",
    "                loss_val = criterion(outputs, labels)\n",
    "                val_loss += loss_val.item() * inputs.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        val_epoch_loss = val_loss / val_total\n",
    "        val_epoch_acc = 100. * val_correct / val_total\n",
    "\n",
    "        metrics[\"epochs\"].append(epoch + 1)\n",
    "        metrics[\"train_loss\"].append(train_epoch_loss)\n",
    "        metrics[\"train_acc\"].append(train_epoch_acc)\n",
    "        metrics[\"val_loss\"].append(val_epoch_loss)\n",
    "        metrics[\"val_acc\"].append(val_epoch_acc)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_epoch_loss:.4f}, Train Acc: {train_epoch_acc:.2f}% | Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.2f}%\")\n",
    "\n",
    "        writer.add_scalar('Loss/Train', train_epoch_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Train', train_epoch_acc, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_epoch_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_epoch_acc, epoch)\n",
    "\n",
    "        if val_epoch_acc > best_val_acc:\n",
    "            best_val_acc = val_epoch_acc\n",
    "            best_epoch = epoch + 1\n",
    "            torch.save(student.state_dict(), best_model_path)\n",
    "            print(f\"New best student model found at epoch {epoch+1} with Val Acc: {val_epoch_acc:.2f}%\")\n",
    "\n",
    "        with open(metrics_file_path, \"w\") as f:\n",
    "            json.dump(metrics, f, indent=4)\n",
    "    \n",
    "    print(\"Training complete.\")\n",
    "    torch.save(student.state_dict(), save_path + \"_final.pth\")\n",
    "\n",
    "writer.close()\n",
    "\n",
    "# Save captured logs.\n",
    "with open(log_file_path, \"w\") as f:\n",
    "    f.write(log_capture.getvalue())\n",
    "\n",
    "print(\"Training log captured and saved to\", log_file_path)\n",
    "print(\"Training metrics saved to\", metrics_file_path)\n",
    "print(\"Best student model saved from epoch\", best_epoch, \"with validation accuracy of\", best_val_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
