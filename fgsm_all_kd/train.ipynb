{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d62241a-012d-4e2c-bbf7-5472114af316",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T21:29:18.384496Z",
     "iopub.status.busy": "2025-04-21T21:29:18.384287Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 21:29:20.595061: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-21 21:29:20.595113: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-21 21:29:20.596253: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-21 21:29:20.602673: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-21 21:29:21.460243: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Generating FGSM adversarial examples for 50000 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating FGSM samples: 100%|██████████| 50000/50000 [11:13<00:00, 74.22it/s] \n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training student (SqueezeNet) with teacher (ResNet18) distillation and FGSM adversarial augmentation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: 100%|██████████| 782/782 [01:09<00:00, 11.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] - Train Loss: 6.6462, Train Acc: 19.38% | Val Loss: 1.9756, Val Acc: 28.65%\n",
      "New best student model found at epoch 1 with Val Acc: 28.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/200: 100%|██████████| 782/782 [01:09<00:00, 11.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/200] - Train Loss: 5.2075, Train Acc: 34.84% | Val Loss: 1.6859, Val Acc: 44.41%\n",
      "New best student model found at epoch 2 with Val Acc: 44.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/200: 100%|██████████| 782/782 [01:08<00:00, 11.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/200] - Train Loss: 4.5989, Train Acc: 42.52% | Val Loss: 1.7597, Val Acc: 48.43%\n",
      "New best student model found at epoch 3 with Val Acc: 48.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/200: 100%|██████████| 782/782 [01:07<00:00, 11.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/200] - Train Loss: 4.2323, Train Acc: 46.88% | Val Loss: 1.7602, Val Acc: 50.63%\n",
      "New best student model found at epoch 4 with Val Acc: 50.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/200: 100%|██████████| 782/782 [01:08<00:00, 11.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/200] - Train Loss: 3.9663, Train Acc: 49.91% | Val Loss: 1.4740, Val Acc: 55.72%\n",
      "New best student model found at epoch 5 with Val Acc: 55.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/200: 100%|██████████| 782/782 [01:08<00:00, 11.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/200] - Train Loss: 3.7343, Train Acc: 52.47% | Val Loss: 1.4482, Val Acc: 59.41%\n",
      "New best student model found at epoch 6 with Val Acc: 59.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/200: 100%|██████████| 782/782 [01:07<00:00, 11.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/200] - Train Loss: 3.5441, Train Acc: 54.48% | Val Loss: 1.2854, Val Acc: 61.48%\n",
      "New best student model found at epoch 7 with Val Acc: 61.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/200: 100%|██████████| 782/782 [01:08<00:00, 11.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/200] - Train Loss: 3.3959, Train Acc: 56.07% | Val Loss: 1.2753, Val Acc: 62.62%\n",
      "New best student model found at epoch 8 with Val Acc: 62.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/200:  51%|█████     | 399/782 [00:35<00:33, 11.28it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Dataset, random_split\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from contextlib import redirect_stdout\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# Logging Utilities\n",
    "# -------------------------------\n",
    "class Tee(object):\n",
    "    def __init__(self, *fileobjects):\n",
    "        self.fileobjects = fileobjects\n",
    "    def write(self, text):\n",
    "        for f in self.fileobjects:\n",
    "            f.write(text)\n",
    "            f.flush()\n",
    "    def flush(self):\n",
    "        for f in self.fileobjects:\n",
    "            f.flush()\n",
    "\n",
    "# -------------------------------\n",
    "# Paths and Checkpoints\n",
    "# -------------------------------\n",
    "log_file_path = \"./logs/cifar10_training_log.txt\"\n",
    "metrics_file_path = \"./metrics/cifar10_training_metrics.json\"\n",
    "best_model_path = \"./models/cifar10_best_student_model.pth\"\n",
    "save_path = \"./models/cifar10_student_model\"\n",
    "teacher_checkpoint = \"/notebooks/Resnet18/models/cifar10_best_model\"  # Pre-trained teacher\n",
    "\n",
    "# -------------------------------\n",
    "# Data Augmentations (No Mixup)\n",
    "# -------------------------------\n",
    "norm = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "# Define three augmentation pipelines for the enriched dataset.\n",
    "transform_a = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    norm\n",
    "])\n",
    "\n",
    "transform_b = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    norm\n",
    "])\n",
    "\n",
    "transform_c = transforms.Compose([\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    norm\n",
    "])\n",
    "\n",
    "# For evaluation and adversarial generation, use a simple transform.\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    norm\n",
    "])\n",
    "\n",
    "# -------------------------------\n",
    "# Load CIFAR-10 Datasets\n",
    "# -------------------------------\n",
    "# Create an enriched dataset using three copies with different augmentations.\n",
    "dataset_a = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_a)\n",
    "dataset_b = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_b)\n",
    "dataset_c = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_c)\n",
    "enriched_dataset = ConcatDataset([dataset_a, dataset_b, dataset_c])\n",
    "\n",
    "# Full training set with eval_transform (for generating adversarial examples).\n",
    "full_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=eval_transform)\n",
    "\n",
    "# Split the full training set for validation.\n",
    "train_size = int(0.8 * len(full_trainset))\n",
    "val_size = len(full_trainset) - train_size\n",
    "trainset_eval, valset = random_split(full_trainset, [train_size, val_size])\n",
    "\n",
    "# Test set.\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=eval_transform)\n",
    "\n",
    "# -------------------------------\n",
    "# FGSM Adversarial Example Generator\n",
    "# -------------------------------\n",
    "def generate_FGSM(image, label, teacher, epsilon=0.01):\n",
    "    \"\"\"\n",
    "    Generates an adversarial example using the FGSM method.\n",
    "    \n",
    "    Args:\n",
    "        image (torch.Tensor): A normalized image tensor (range [-1,1]).\n",
    "        label (int): The true label of the image.\n",
    "        teacher (torch.nn.Module): The teacher model.\n",
    "        epsilon (float): Perturbation magnitude.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: The adversarial image.\n",
    "    \"\"\"\n",
    "    # Clone the image and enable gradient computation.\n",
    "    image = image.clone().detach().to(device)\n",
    "    image.requires_grad = True\n",
    "\n",
    "    # Forward pass.\n",
    "    teacher.zero_grad()\n",
    "    logits = teacher(image.unsqueeze(0))  # add batch dimension\n",
    "\n",
    "    # Compute loss using the ground-truth label.\n",
    "    loss = nn.CrossEntropyLoss()(logits, torch.tensor([label]).to(device))\n",
    "    loss.backward()\n",
    "    # Get the sign of the gradients.\n",
    "    data_grad = image.grad.data\n",
    "    # Create the adversarial example by perturbing the image.\n",
    "    perturbed_image = image + epsilon * data_grad.sign()\n",
    "    # Ensure the perturbed image remains in the valid range (for normalized images, this is [-1, 1]).\n",
    "    perturbed_image = torch.clamp(perturbed_image, -1, 1)\n",
    "    \n",
    "    return perturbed_image.detach()\n",
    "\n",
    "# -------------------------------\n",
    "# Prepare the Teacher Model (ResNet18 for CIFAR-10)\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "teacher = models.resnet18(weights=None)\n",
    "# Modify the first convolution and maxpool for CIFAR-10.\n",
    "teacher.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "teacher.maxpool = nn.Identity()\n",
    "num_ftrs = teacher.fc.in_features\n",
    "teacher.fc = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "# Load teacher checkpoint if it exists.\n",
    "if os.path.exists(teacher_checkpoint):\n",
    "    teacher_state = torch.load(teacher_checkpoint, map_location=device)\n",
    "    teacher.load_state_dict(teacher_state)\n",
    "else:\n",
    "    print(\"Warning: Teacher checkpoint not found. Using an untrained teacher model.\")\n",
    "\n",
    "teacher.to(device)\n",
    "teacher.eval()\n",
    "\n",
    "# -------------------------------\n",
    "# Create FGSM-Based Adversarial Dataset\n",
    "# -------------------------------\n",
    "# Determine the fraction of full_trainset to perturb.\n",
    "adv_fraction = 1  \n",
    "num_adv_samples = int(len(full_trainset) * adv_fraction)\n",
    "print(f\"Generating FGSM adversarial examples for {num_adv_samples} samples...\")\n",
    "\n",
    "adv_samples = []\n",
    "# Randomly select indices for FGSM adversarial generation.\n",
    "selected_indices = np.random.choice(len(full_trainset), num_adv_samples, replace=False)\n",
    "# FGSM epsilon value (tunable).\n",
    "fgsm_epsilon = 0.01\n",
    "\n",
    "for idx in tqdm(selected_indices, desc=\"Generating FGSM samples\"):\n",
    "    image, label = full_trainset[idx]\n",
    "    adv_image = generate_FGSM(image, label, teacher, epsilon=fgsm_epsilon)\n",
    "    # Keep the original label.\n",
    "    adv_samples.append((adv_image.cpu(), label))\n",
    "\n",
    "# Wrap the adversarial examples in a simple Dataset.\n",
    "class AdversarialDataset(Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = samples\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]  # returns (image, label)\n",
    "\n",
    "adv_dataset = AdversarialDataset(adv_samples)\n",
    "# Combine enriched (augmented) dataset with FGSM adversarial examples.\n",
    "combined_dataset = ConcatDataset([enriched_dataset, adv_dataset])\n",
    "\n",
    "# -------------------------------\n",
    "# DataLoaders (No Mixup)\n",
    "# -------------------------------\n",
    "train_loader = DataLoader(combined_dataset, batch_size=256, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(valset, batch_size=256, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(testset, batch_size=256, shuffle=False, num_workers=2)\n",
    "\n",
    "# -------------------------------\n",
    "# Prepare Teacher and Student Models for Distillation\n",
    "# -------------------------------\n",
    "# The teacher model is already prepared above and remains frozen.\n",
    "# Prepare the student model (SqueezeNet1_1 adapted for CIFAR-10).\n",
    "student = models.squeezenet1_1(pretrained=False)\n",
    "student.classifier[1] = nn.Conv2d(512, 10, kernel_size=(1,1), stride=(1,1))\n",
    "student.num_classes = 10\n",
    "student.to(device)\n",
    "\n",
    "# Optionally resume student training from a checkpoint.\n",
    "start_checkpoint = None\n",
    "if start_checkpoint:\n",
    "    student_state = torch.load(start_checkpoint, map_location=device)\n",
    "    student.load_state_dict(student_state)\n",
    "\n",
    "# Freeze teacher parameters.\n",
    "for param in teacher.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# -------------------------------\n",
    "# Training Hyperparameters and Loss Setup\n",
    "# -------------------------------\n",
    "num_epochs = 200\n",
    "learning_rate = 0.001\n",
    "checkpoint_frequency = 10\n",
    "\n",
    "# Distillation loss: KL divergence with temperature scaling.\n",
    "kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(student.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "temperature = 4.0\n",
    "\n",
    "# CIFAR-10 class names (for logging if desired).\n",
    "cifar_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# -------------------------------\n",
    "# Metrics and Logging Setup\n",
    "# -------------------------------\n",
    "metrics = {\n",
    "    \"epochs\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": []\n",
    "}\n",
    "best_val_acc = 0.0\n",
    "best_epoch = None\n",
    "log_capture = io.StringIO()\n",
    "tee = Tee(sys.stdout, log_capture)\n",
    "writer = SummaryWriter(log_dir='./runs/cifar10_distillation_experiment')\n",
    "\n",
    "# -------------------------------\n",
    "# Training Loop: Knowledge Distillation with FGSM Augmentation\n",
    "# -------------------------------\n",
    "with redirect_stdout(tee):\n",
    "    print(\"Training student (SqueezeNet) with teacher (ResNet18) distillation and FGSM adversarial augmentation...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        student.train()\n",
    "        running_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        if epoch % checkpoint_frequency == 0:\n",
    "            torch.save(student.state_dict(), f\"{save_path}_{epoch}.pth\")\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass through teacher (for soft targets).\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(inputs)\n",
    "            # Forward pass through student.\n",
    "            student_logits = student(inputs)\n",
    "\n",
    "            # Compute distillation loss using temperature-scaled KL divergence.\n",
    "            teacher_soft = torch.softmax(teacher_logits / temperature, dim=1)\n",
    "            student_log_soft = torch.log_softmax(student_logits / temperature, dim=1)\n",
    "            loss_distill = (temperature ** 2) * kl_loss(student_log_soft, teacher_soft)\n",
    "            loss = loss_distill\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            total += inputs.size(0)\n",
    "            _, predicted = student_logits.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        train_epoch_loss = running_loss / total\n",
    "        train_epoch_acc = 100. * correct / total\n",
    "\n",
    "        # ----- Validation Loop -----\n",
    "        student.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = student(inputs)\n",
    "                loss_val = criterion(outputs, labels)\n",
    "                val_loss += loss_val.item() * inputs.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        val_epoch_loss = val_loss / val_total\n",
    "        val_epoch_acc = 100. * val_correct / val_total\n",
    "\n",
    "        metrics[\"epochs\"].append(epoch + 1)\n",
    "        metrics[\"train_loss\"].append(train_epoch_loss)\n",
    "        metrics[\"train_acc\"].append(train_epoch_acc)\n",
    "        metrics[\"val_loss\"].append(val_epoch_loss)\n",
    "        metrics[\"val_acc\"].append(val_epoch_acc)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_epoch_loss:.4f}, Train Acc: {train_epoch_acc:.2f}% | Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.2f}%\")\n",
    "        writer.add_scalar('Loss/Train', train_epoch_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Train', train_epoch_acc, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_epoch_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_epoch_acc, epoch)\n",
    "\n",
    "        if val_epoch_acc > best_val_acc:\n",
    "            best_val_acc = val_epoch_acc\n",
    "            best_epoch = epoch + 1\n",
    "            torch.save(student.state_dict(), best_model_path)\n",
    "            print(f\"New best student model found at epoch {epoch+1} with Val Acc: {val_epoch_acc:.2f}%\")\n",
    "\n",
    "        with open(metrics_file_path, \"w\") as f:\n",
    "            json.dump(metrics, f, indent=4)\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    torch.save(student.state_dict(), save_path + \"_final.pth\")\n",
    "\n",
    "writer.close()\n",
    "\n",
    "# Save captured logs.\n",
    "with open(log_file_path, \"w\") as f:\n",
    "    f.write(log_capture.getvalue())\n",
    "\n",
    "print(\"Training log captured and saved to\", log_file_path)\n",
    "print(\"Training metrics saved to\", metrics_file_path)\n",
    "print(\"Best student model saved from epoch\", best_epoch, \"with validation accuracy of\", best_val_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
